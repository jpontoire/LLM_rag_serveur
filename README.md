# ğŸ§  Serveur RAG (Retrieval-Augmented Generation)

ImplÃ©mentation d'un serveur LLM (`RAG`) en Python.

---

## ğŸ“‚ Structure

```
ğŸ“ RAG/
â”‚   â”œâ”€â”€ init_rag.py         # Script d'initialisation du moteur RAG
â”‚   â”œâ”€â”€ run.py              # Run serveur avec uvicorn
â”‚   â””â”€â”€ server_rag.py       # FastAPI avec endpoint POST /query
â”‚
ğŸ“ LOGS/
â”‚   â””â”€â”€ SERVER_MISTRAL/     # Dossier oÃ¹ sont enregistrÃ©s les logs dâ€™exÃ©cution et rÃ©ponses
â”‚
ğŸ“ DATA/                    # Documents support du rag (.pdf, .txt, .csv)
â”‚
runRAG.bat                 # Script pour lancer le serveur sur Windows
requirements.txt           # Liste des dÃ©pendances Python
README.md                 
```

---

## âš™ï¸ Fonctionnement

1. **Chargement des documents**
   - Les fichiers dans `DATA/` sont chargÃ©s.
   - Les extensions prises en charge sont : `.txt`, `.pdf`, `.csv`.

2. **DÃ©coupage en chunks**
   - Les documents sont dÃ©coupÃ©s en blocs de K chunks de taille fixÃ©e.

3. **Vectorisation**
   - Chaque chunk est converti en vecteur via un modÃ¨le d'embedding choisi.

4. **Indexation FAISS**
   - Tous les vecteurs sont stockÃ©s dans un index FAISS (peut prendre du temps).

5. **RequÃªte**
   - L'utilisateur envoie une question en POST via l'API.
   - Le systÃ¨me interroge les chunks les plus pertinents et gÃ©nÃ¨re une rÃ©ponse Ã  l'aide du modÃ¨le Ollama (`mistral-small` par dÃ©faut).

---

## ğŸš€ Installation et dÃ©marrage

### 1. PrÃ©requis

- Python 3.10+
- [Ollama](https://ollama.com/) installÃ© localement
- ModÃ¨le Ollama tÃ©lÃ©chargÃ© (`mistral-small:24b`, ou autre)
- ModÃ¨le pour l'EMBEDDING (modÃ¨les disponibles sur https://huggingface.co/models?other=text-embeddings-inference)

Exemples :
- BAAI/bge-m3
- nomic-ai/nomic-embed-text-v1.5

TÃ©lÃ©charger un modÃ¨le sur Ollama, via terminal :
```bash
ollama pull mistral-small
```

### 2. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

### 3. Lancer le serveur

#### Via le script Windows `runRAG.bat` :
```bat
@echo off
echo ===============================
echo Starting RAG server on port 8000
echo ===============================
python RAG/run.py --modelembedding "nomic-ai/nomic-embed-text-v1.5" --kchunk 4 --sizechunk 4000
pause
```

Ce script utilise :
- ModÃ¨le d'embedding : `nomic-ai/nomic-embed-text-v1.5`
- Nombre de chunks pertinents Ã  extraire (`kchunk`) : 4
- Taille maximale dâ€™un chunk (`sizechunk`) : 4000 caractÃ¨res

*Les modÃ¨les d'embedding sont tÃ©lÃ©chargÃ©s localement puis les chunks sont dÃ©coupÃ©s et sauvegardÃ©s localement Ã©galement. 
NÃ©cessite du temps selon le dÃ©coupage et le modÃ¨le choisi (plusieurs minutes voire heures).*

---

## ğŸ§ª Utilisation de lâ€™API

### ğŸ”— Endpoint

```http
POST /query
```

### ğŸ” Corps de la requÃªte

```json
{
  "prompt": "Quelles sont les informations Ã  la date du 21 mai 1944 ?"
}
```

### âœ… RÃ©ponse

```json
{
  "answer": "Texte de rÃ©ponse gÃ©nÃ©rÃ© Ã  partir des documents.",
  "execution_time_sec": 1.58
}
```

---

## ğŸ“¦ Exemple de test avec `curl`

```bash
curl -X POST http://127.0.0.1:8000/query \
     -H "Content-Type: application/json" \
     -d "{\"prompt\": \"Quelles sont les informations Ã  la date du 21 mai 1944 ?\"}"
```

---
